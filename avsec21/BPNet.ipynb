{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import eugene as eu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProfileDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import pyfaidx\n",
    "import pyBigWig\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_loci(\n",
    "\tloci, \n",
    "    sequences, \n",
    "    signals=None, \n",
    "    controls=None, \n",
    "    chroms=None, \n",
    "\tin_window=2114,\n",
    "    out_window=1000, \n",
    "    max_jitter=128, \n",
    "    min_counts=None,\n",
    "\tmax_counts=None, \n",
    "    verbose=False\n",
    "):\n",
    "\t\"\"\"Extract sequences and signals at coordinates from a locus file.\n",
    "\tThis function will take in genome-wide sequences, signals, and optionally\n",
    "\tcontrols, and extract the values of each at the coordinates specified in\n",
    "\tthe locus file/s and return them as tensors.\n",
    "\tSignals and controls are both lists with the length of the list, n_s\n",
    "\tand n_c respectively, being the middle dimension of the returned\n",
    "\ttensors. Specifically, the returned tensors of size \n",
    "\t(len(loci), n_s/n_c, (out_window/in_wndow)+max_jitter*2).\n",
    "\tThe values for sequences, signals, and controls, can either be filepaths\n",
    "\tor dictionaries of np arrays or a mix of the two. When a filepath is \n",
    "\tpassed in it is loaded using pyfaidx or pyBigWig respectively.   \n",
    "\tParameters\n",
    "\t----------\n",
    "\tloci: str or pd.DataFrame or list/tuple of such\n",
    "\t\tEither the path to a bed file or a pd DataFrame object containing\n",
    "\t\tthree columns: the chromosome, the start, and the end, of each locus\n",
    "\t\tto train on. Alternatively, a list or tuple of strings/DataFrames where\n",
    "\t\tthe intention is to train on the interleaved concatenation, i.e., when\n",
    "\t\tyou want to train on peaks and negatives.\n",
    "\tsequences: str or dictionary\n",
    "\t\tEither the path to a fasta file to read from or a dictionary where the\n",
    "\t\tkeys are the unique set of chromosoms and the values are one-hot\n",
    "\t\tencoded sequences as np arrays or memory maps.\n",
    "\tsignals: list of strs or list of dictionaries or None, optional\n",
    "\t\tA list of filepaths to bigwig files, where each filepath will be read\n",
    "\t\tusing pyBigWig, or a list of dictionaries where the keys are the same\n",
    "\t\tset of unique chromosomes and the values are np arrays or memory\n",
    "\t\tmaps. If None, no signal tensor is returned. Default is None.\n",
    "\tcontrols: list of strs or list of dictionaries or None, optional\n",
    "\t\tA list of filepaths to bigwig files, where each filepath will be read\n",
    "\t\tusing pyBigWig, or a list of dictionaries where the keys are the same\n",
    "\t\tset of unique chromosomes and the values are np arrays or memory\n",
    "\t\tmaps. If None, no control tensor is returned. Default is None. \n",
    "\tchroms: list or None, optional\n",
    "\t\tA set of chromosomes to extact loci from. Loci in other chromosomes\n",
    "\t\tin the locus file are ignored. If None, all loci are used. Default is\n",
    "\t\tNone.\n",
    "\tin_window: int, optional\n",
    "\t\tThe input window size. Default is 2114.\n",
    "\tout_window: int, optional\n",
    "\t\tThe output window size. Default is 1000.\n",
    "\tmax_jitter: int, optional\n",
    "\t\tThe maximum amount of jitter to add, in either direction, to the\n",
    "\t\tmidpoints that are passed in. Default is 128.\n",
    "\tmin_counts: float or None, optional\n",
    "\t\tThe minimum number of counts, summed across the length of each example\n",
    "\t\tand across all tasks, needed to be kept. If None, no minimum. Default \n",
    "\t\tis None.\n",
    "\tmax_counts: float or None, optional\n",
    "\t\tThe maximum number of counts, summed across the length of each example\n",
    "\t\tand across all tasks, needed to be kept. If None, no maximum. Default \n",
    "\t\tis None.  \n",
    "\tverbose: bool, optional\n",
    "\t\tWhether to display a progress bar while loading. Default is False.\n",
    "\tReturns\n",
    "\t-------\n",
    "\tseqs: torch.tensor, shape=(n, 4, in_window+2*max_jitter)\n",
    "\t\tThe extracted sequences in the same order as the loci in the locus\n",
    "\t\tfile after optional filtering by chromosome.\n",
    "\tsignals: torch.tensor, shape=(n, len(signals), out_window+2*max_jitter)\n",
    "\t\tThe extracted signals where the first dimension is in the same order\n",
    "\t\tas loci in the locus file after optional filtering by chromosome and\n",
    "\t\tthe second dimension is in the same order as the list of signal files.\n",
    "\t\tIf no signal files are given, this is not returned.\n",
    "\tcontrols: torch.tensor, shape=(n, len(controls), out_window+2*max_jitter)\n",
    "\t\tThe extracted controls where the first dimension is in the same order\n",
    "\t\tas loci in the locus file after optional filtering by chromosome and\n",
    "\t\tthe second dimension is in the same order as the list of control files.\n",
    "\t\tIf no control files are given, this is not returned.\n",
    "\t\"\"\"\n",
    "\n",
    "\tseqs, signals_, controls_ = [], [], []\n",
    "\tin_width, out_width = in_window // 2, out_window // 2\n",
    "\n",
    "\t# Load the sequences\n",
    "\tif isinstance(sequences, str):\n",
    "\t\tsequences = pyfaidx.Fasta(sequences)\n",
    "\n",
    "\tnames = ['chrom', 'start', 'end']\n",
    "\tif not isinstance(loci, (tuple, list)):\n",
    "\t\tloci = [loci]\n",
    "\n",
    "\tloci_dfs = []\n",
    "\tfor i, df in enumerate(loci):\n",
    "\t\tif isinstance(df, str):\n",
    "\t\t\tdf = pd.read_csv(df, sep='\\t', usecols=[0, 1, 2], header=None, index_col=False, names=names)\n",
    "\t\t\tdf['idx'] = np.arange(len(df)) * len(loci) + i\n",
    "\t\tloci_dfs.append(df)\n",
    "\n",
    "\tloci = pd.concat(loci_dfs).set_index(\"idx\").sort_index().reset_index(drop=True)\n",
    "\tif chroms is not None:\n",
    "\t\tloci = loci[np.isin(loci['chrom'], chroms)]\n",
    "\n",
    "\t# Load the signal and optional control tracks if filenames are given\n",
    "\tif signals is not None:\n",
    "\t\tfor i, signal in enumerate(signals):\n",
    "\t\t\tif isinstance(signal, str):\n",
    "\t\t\t\tsignals[i] = pyBigWig.open(signal, \"r\")\n",
    "\n",
    "\tif controls is not None:\n",
    "\t\tfor i, control in enumerate(controls):\n",
    "\t\t\tif isinstance(control, str):\n",
    "\t\t\t\tcontrols[i] = pyBigWig.open(control, \"r\")\n",
    "\n",
    "\tdesc = \"Loading Loci\"\n",
    "\td = not verbose\n",
    "\n",
    "\tmax_width = max(in_width, out_width)\n",
    "\n",
    "\tfor chrom, start, end in tqdm(loci.values, disable=d, desc=desc):\n",
    "\t\tmid = start + (end - start) // 2\n",
    "\n",
    "\t\tif start - max_width - max_jitter < 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif end + max_width + max_jitter >= len(sequences[chrom]):\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tstart = mid - out_width - max_jitter\n",
    "\t\tend = mid + out_width + max_jitter\n",
    "\t\t\n",
    "\t\t# Extract the signal from each of the signal files\n",
    "\t\tif signals is not None:\n",
    "\t\t\tsignals_.append([])\n",
    "\t\t\tfor signal in signals:\n",
    "\t\t\t\tif isinstance(signal, dict):\n",
    "\t\t\t\t\tsignal_ = signal[chrom][start:end]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsignal_ = signal.values(chrom, start, end, numpy=True)\n",
    "\t\t\t\t\tsignal_ = np.nan_to_num(signal_)\n",
    "\n",
    "\t\t\t\tsignals_[-1].append(signal_)\n",
    "\n",
    "\t\t# For the sequences and controls extract a window the size of the input\n",
    "\t\tstart = mid - in_width - max_jitter\n",
    "\t\tend = mid + in_width + max_jitter\n",
    "\n",
    "\t\t# Extract the controls from each of the control files\n",
    "\t\tif controls is not None:\n",
    "\t\t\tcontrols_.append([])\n",
    "\t\t\tfor control in controls:\n",
    "\t\t\t\tif isinstance(control, dict):\n",
    "\t\t\t\t\tcontrol_ = control[chrom][start:end]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontrol_ = control.values(chrom, start, end, numpy=True)\n",
    "\t\t\t\t\tcontrol_ = np.nan_to_num(control_)\n",
    "\n",
    "\t\t\t\tcontrols_[-1].append(control_)\n",
    "\n",
    "\t\t# Extract the sequence\n",
    "\t\tif isinstance(sequences, dict):\n",
    "\t\t\tseq = sequences[chrom][start:end].T\n",
    "\t\telse:\n",
    "\t\t\tseq = eu.pp.ohe_seq(sequences[chrom][start:end].seq.upper())\n",
    "\t\t\n",
    "\t\tseqs.append(seq)\n",
    "\n",
    "\tseqs = torch.tensor(np.array(seqs), dtype=torch.float32)\n",
    "\n",
    "\tif signals is not None:\n",
    "\t\tsignals_ = torch.tensor(np.array(signals_), dtype=torch.float32)\n",
    "\n",
    "\t\tidxs = torch.ones(signals_.shape[0], dtype=torch.bool)\n",
    "\t\tif max_counts is not None:\n",
    "\t\t\tidxs = (idxs) & (signals_.sum(dim=(1, 2)) < max_counts)\n",
    "\t\tif min_counts is not None:\n",
    "\t\t\tidxs = (idxs) & (signals_.sum(dim=(1, 2)) > min_counts)\n",
    "\n",
    "\t\tif controls is not None:\n",
    "\t\t\tcontrols_ = torch.tensor(np.array(controls_), dtype=torch.float32)\n",
    "\t\t\treturn seqs[idxs], signals_[idxs], controls_[idxs]\n",
    "\n",
    "\t\treturn seqs[idxs], signals_[idxs]\n",
    "\telse:\n",
    "\t\tif controls is not None:\n",
    "\t\t\tcontrols_ = torch.tensor(np.array(controls_), dtype=torch.float32)\n",
    "\t\t\treturn seqs, controls_\n",
    "\n",
    "\t\treturn seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"/cellar/users/aklie/data/eugene/avsec21/ENCSR000EGM/data\"\n",
    "reference_dir = \"/cellar/users/aklie/data/eugene/avsec21/reference\"\n",
    "peaks = os.path.join(data_dir, \"peaks.bed\")\n",
    "#peaks = \"/cellar/users/aklie/data/eugene/avsec21/ENCSR000EGM/toy.bed\"\n",
    "seqs = os.path.join(reference_dir, \"hg38.fa\")\n",
    "signals = [os.path.join(data_dir, \"plus.bw\"), os.path.join(data_dir, \"minus.bw\")]\n",
    "controls = [os.path.join(data_dir, \"control_plus.bw\"), os.path.join(data_dir, \"control_minus.bw\")]\n",
    "\n",
    "training_chroms = ['chr{}'.format(i) for i in range(1, 17)]\n",
    "valid_chroms = ['chr{}'.format(i) for i in range(18, 23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy, y_toy, X_ctl_toy = extract_loci(peaks, seqs, signals, controls, max_jitter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 4, 2114]),\n",
       " torch.Size([100, 2, 1000]),\n",
       " torch.Size([100, 2, 2114]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy.shape, y_toy.shape, X_ctl_toy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ProfileDataset(Dataset):\n",
    "\t\"\"\"A data generator for BPNet inputs.\n",
    "\tThis generator takes in an extracted set of sequences, output signals,\n",
    "\tand control signals, and will return a single element with random\n",
    "\tjitter and reverse-complement augmentation applied. Jitter is implemented\n",
    "\tefficiently by taking in data that is wider than the in/out windows by\n",
    "\ttwo times the maximum jitter and windows are extracted from that.\n",
    "\tEssentially, if an input window is 1000 and the maximum jitter is 128, one\n",
    "\twould pass in data with a length of 1256 and a length 1000 window would be\n",
    "\textracted starting between position 0 and 256. This  generator must be \n",
    "\twrapped by a PyTorch generator object.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tsequences: torch.tensor, shape=(n, 4, in_window+2*max_jitter)\n",
    "\t\tA one-hot encoded tensor of `n` example sequences, each of input \n",
    "\t\tlength `in_window`. See description above for connection with jitter.\n",
    "\tsignals: torch.tensor, shape=(n, t, out_window+2*max_jitter)\n",
    "\t\tThe signals to predict, usually counts, for `n` examples with\n",
    "\t\t`t` output tasks (usually 2 if stranded, 1 otherwise), each of \n",
    "\t\toutput length `out_window`. See description above for connection \n",
    "\t\twith jitter.\n",
    "\tcontrols: torch.tensor, shape=(n, t, out_window+2*max_jitter) or None, optional\n",
    "\t\tThe control signal to take as input, usually counts, for `n`\n",
    "\t\texamples with `t` strands and output length `out_window`. If\n",
    "\t\tNone, does not return controls.\n",
    "\tin_window: int, optional\n",
    "\t\tThe input window size. Default is 2114.\n",
    "\tout_window: int, optional\n",
    "\t\tThe output window size. Default is 1000.\n",
    "\tmax_jitter: int, optional\n",
    "\t\tThe maximum amount of jitter to add, in either direction, to the\n",
    "\t\tmidpoints that are passed in. Default is 0.\n",
    "\treverse_complement: bool, optional\n",
    "\t\tWhether to reverse complement-augment half of the data. Default is False.\n",
    "\trandom_state: int or None, optional\n",
    "\t\tWhether to use a deterministic seed or not.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tsequences, \n",
    "\t\tsignals, \n",
    "\t\tcontrols=None, \n",
    "\t\tin_window=2114, \n",
    "\t\tout_window=1000, \n",
    "\t\tmax_jitter=0, \n",
    "\t\treverse_complement=False, \n",
    "\t\trandom_state=None\n",
    "\t):\n",
    "\t\tself.in_window = in_window\n",
    "\t\tself.out_window = out_window\n",
    "\t\tself.max_jitter = max_jitter\n",
    "\t\t\n",
    "\t\tself.reverse_complement = reverse_complement\n",
    "\t\tself.random_state = np.random.RandomState(random_state)\n",
    "\n",
    "\t\tself.signals = signals\n",
    "\t\tself.controls = controls\n",
    "\t\tself.sequences = sequences\t\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.sequences)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t#i = self.random_state.choice(len(self.sequences))\n",
    "\t\tj = 0 if self.max_jitter == 0 else self.random_state.randint(self.max_jitter*2) \n",
    "\n",
    "\t\tX = self.sequences[idx][:, j:j+self.in_window]\n",
    "\t\ty = self.signals[idx][:, j:j+self.out_window]\n",
    "\n",
    "\t\tif self.controls is not None:\n",
    "\t\t\tX_ctl = self.controls[idx][:, j:j+self.in_window]\n",
    "\n",
    "\t\tif self.reverse_complement and self.random_state.choice(2) == 1:\n",
    "\t\t\tX = torch.flip(X, [0, 1])\n",
    "\t\t\ty = torch.flip(y, [0, 1])\n",
    "\n",
    "\t\t\tif self.controls is not None:\n",
    "\t\t\t\tX_ctl = torch.flip(X_ctl, [0, 1])\n",
    "\n",
    "\t\tif self.controls is not None:\n",
    "\t\t\treturn X, X_ctl, y\n",
    "\n",
    "\t\treturn X, y\n",
    "\t\n",
    "\tdef to_dataloader(\n",
    "\t\tself, \n",
    "        batch_size=None, \n",
    "        pin_memory=True, \n",
    "        shuffle=True, \n",
    "        num_workers=0, \n",
    "        **kwargs\n",
    "    ):\n",
    "\t\t\"\"\"Convert the dataset to a PyTorch DataLoader\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t----------\n",
    "\t\tbatch_size (int, optional):\n",
    "\t\t\tbatch size for dataloader\n",
    "\t\tpin_memory (bool, optional):\n",
    "\t\t\twhether to pin memory for dataloader\n",
    "\t\tshuffle (bool, optional):\n",
    "\t\t\twhether to shuffle the dataset\n",
    "\t\tnum_workers (int, optional):\n",
    "\t\t\tnumber of workers for dataloader\n",
    "\t\t**kwargs:\n",
    "\t\t\tadditional arguments to pass to DataLoader\n",
    "\t\t\"\"\"\n",
    "\t\tbatch_size = batch_size if batch_size is not None else eu.settings.batch_size\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tpin_memory=pin_memory,\n",
    "\t\t\tshuffle=shuffle,\n",
    "\t\t\tnum_workers=num_workers,\n",
    "\t\t\t**kwargs\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataset = ProfileDataset(\n",
    "    X_toy,\n",
    "    y_toy,\n",
    "    X_ctl_toy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2114]), torch.Size([2, 2114]), torch.Size([2, 1000]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_dataset[0][0].shape, toy_dataset[0][1].shape, toy_dataset[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_dataloader = toy_dataset.to_dataloader(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 4, 2114]),\n",
       " torch.Size([100, 2, 2114]),\n",
       " torch.Size([100, 2, 1000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(toy_dataloader))\n",
    "batch[0].shape, batch[1].shape, batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "         [1., 1., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         [1., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 1., 0.]],\n",
       "\n",
       "        [[1., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpnetlite.io import PeakGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpnetlite_dataloader = PeakGenerator(\n",
    "    peaks,\n",
    "    seqs,\n",
    "    signals,\n",
    "    controls,\n",
    "    max_jitter=0,\n",
    "    batch_size=128,\n",
    "    random_state=13\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 4, 2114]),\n",
       " torch.Size([100, 2, 2114]),\n",
       " torch.Size([100, 2, 1000]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpnetlite_batch = next(iter(bpnetlite_dataloader))\n",
    "bpnetlite_batch[0].shape, bpnetlite_batch[1].shape, bpnetlite_batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 1., 0.],\n",
       "         [1., 0., 0.,  ..., 1., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
       "         [1., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 1., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [1., 1., 1.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpnetlite_batch[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpnetlite\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class BPNet(torch.nn.Module):\n",
    "\t\"\"\"A basic BPNet model with stranded profile and total count prediction.\n",
    "\tThis is a reference implementation for BPNet. The model takes in\n",
    "\tone-hot encoded sequence, runs it through: \n",
    "\t(1) a single wide convolution operation \n",
    "\tTHEN \n",
    "\t(2) a user-defined number of dilated residual convolutions\n",
    "\tTHEN\n",
    "\t(3a) profile predictions done using a very wide convolution layer \n",
    "\tthat also takes in stranded control tracks \n",
    "\tAND\n",
    "\t(3b) total count prediction done using an average pooling on the output\n",
    "\tfrom 2 followed by concatenation with the log1p of the sum of the\n",
    "\tstranded control tracks and then run through a dense layer.\n",
    "\tThis implementation differs from the original BPNet implementation in\n",
    "\ttwo ways:\n",
    "\t(1) The model concatenates stranded control tracks for profile\n",
    "\tprediction as opposed to adding the two strands together and also then\n",
    "\tsmoothing that track \n",
    "\t(2) The control input for the count prediction task is the log1p of\n",
    "\tthe strand-wise sum of the control tracks, as opposed to the raw\n",
    "\tcounts themselves.\n",
    "\t(3) A single log softmax is applied across both strands such that\n",
    "\tthe logsumexp of both strands together is 0. Put another way, the\n",
    "\ttwo strands are concatenated together, a log softmax is applied,\n",
    "\tand the MNLL loss is calculated on the concatenation. \n",
    "\t(4) The count prediction task is predicting the total counts across\n",
    "\tboth strands. The counts are then distributed across strands according\n",
    "\tto the single log softmax from 3.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tn_filters: int, optional\n",
    "\t\tThe number of filters to use per convolution. Default is 64.\n",
    "\tn_layers: int, optional\n",
    "\t\tThe number of dilated residual layers to include in the model.\n",
    "\t\tDefault is 8.\n",
    "\tn_outputs: int, optional\n",
    "\t\tThe number of profile outputs from the model. Generally either 1 or 2 \n",
    "\t\tdepending on if the data is unstranded or stranded. Default is 2.\n",
    "\talpha: float, optional\n",
    "\t\tThe weight to put on the count loss.\n",
    "\tname: str or None, optional\n",
    "\t\tThe name to save the model to during training.\n",
    "\ttrimming: int or None, optional\n",
    "\t\tThe amount to trim from both sides of the input window to get the\n",
    "\t\toutput window. This value is removed from both sides, so the total\n",
    "\t\tnumber of positions removed is 2*trimming.\n",
    "\tverbose: bool, optional\n",
    "\t\tWhether to display statistics during training. Setting this to False\n",
    "\t\twill still save the file at the end, but does not print anything to\n",
    "\t\tscreen during training. Default is True.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, n_filters=64, n_layers=8, n_outputs=2, \n",
    "\t\tn_control_tracks=2, alpha=1, profile_output_bias=True, \n",
    "\t\tcount_output_bias=True, name=None, trimming=None, verbose=True):\n",
    "\t\tsuper(BPNet, self).__init__()\n",
    "\t\tself.n_filters = n_filters\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.n_outputs = n_outputs\n",
    "\t\tself.n_control_tracks = n_control_tracks\n",
    "\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.name = name or \"bpnet.{}.{}\".format(n_filters, n_layers)\n",
    "\t\tself.trimming = trimming or 2 ** n_layers\n",
    "\n",
    "\t\tself.iconv = torch.nn.Conv1d(4, n_filters, kernel_size=21, padding=10)\n",
    "\t\tself.irelu = torch.nn.ReLU()\n",
    "\n",
    "\t\tself.rconvs = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=3, padding=2**i, \n",
    "\t\t\t\tdilation=2**i) for i in range(1, self.n_layers+1)\n",
    "\t\t])\n",
    "\t\tself.rrelus = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.ReLU() for i in range(1, self.n_layers+1)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.fconv = torch.nn.Conv1d(n_filters+n_control_tracks, n_outputs, \n",
    "\t\t\tkernel_size=75, padding=37, bias=profile_output_bias)\n",
    "\t\t\n",
    "\t\tn_count_control = 1 if n_control_tracks > 0 else 0\n",
    "\t\tself.linear = torch.nn.Linear(n_filters+n_count_control, 1, \n",
    "\t\t\tbias=count_output_bias)\n",
    "\n",
    "\tdef forward(self, X, X_ctl=None):\n",
    "\t\t\"\"\"A forward pass of the model.\n",
    "\t\tThis method takes in a nucleotide sequence X, a corresponding\n",
    "\t\tper-position value from a control track, and a per-locus value\n",
    "\t\tfrom the control track and makes predictions for the profile \n",
    "\t\tand for the counts. This per-locus value is usually the\n",
    "\t\tlog(sum(X_ctl_profile)+1) when the control is an experimental\n",
    "\t\tread track but can also be the output from another model.\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tX: torch.tensor, shape=(batch_size, 4, sequence_length)\n",
    "\t\t\tThe one-hot encoded batch of sequences.\n",
    "\t\tX_ctl: torch.tensor, shape=(batch_size, n_strands, sequence_length)\n",
    "\t\t\tA value representing the signal of the control at each position in \n",
    "\t\t\tthe sequence.\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\ty_profile: torch.tensor, shape=(batch_size, n_strands, out_length)\n",
    "\t\t\tThe output predictions for each strand.\n",
    "\t\t\"\"\"\n",
    "\t\tstart, end = self.trimming, X.shape[2] - self.trimming\n",
    "\n",
    "\t\tX = self.irelu(self.iconv(X))\n",
    "\t\tfor i in range(self.n_layers):\n",
    "\t\t\tX_conv = self.rrelus[i](self.rconvs[i](X))\n",
    "\t\t\tX = torch.add(X, X_conv)\n",
    "\n",
    "\t\tif X_ctl is None:\n",
    "\t\t\tX_w_ctl = X\n",
    "\t\telse:\n",
    "\t\t\tX_w_ctl = torch.cat([X, X_ctl], dim=1)\n",
    "\n",
    "\t\ty_profile = self.fconv(X_w_ctl)[:, :, start:end]\n",
    "\n",
    "\t\t# counts prediction\n",
    "\t\tX = torch.mean(X[:, :, start-37:end+37], dim=2)\n",
    "\n",
    "\t\tif X_ctl is not None:\n",
    "\t\t\tX_ctl = torch.sum(X_ctl[:, :, start-37:end+37], dim=(1, 2))\n",
    "\t\t\tX_ctl = X_ctl.unsqueeze(-1)\n",
    "\t\t\tX = torch.cat([X, torch.log(X_ctl+1)], dim=-1)\n",
    "\n",
    "\t\ty_counts = self.linear(X).reshape(X.shape[0], 1)\n",
    "\t\treturn y_profile, y_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BPNet(n_outputs=2, n_control_tracks=2, trimming=(2114 - 1000) // 2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BPNet                                    [32, 2, 1000]             --\n",
       "├─Conv1d: 1-1                            [32, 64, 2114]            5,440\n",
       "├─ReLU: 1-2                              [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-1                       [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-2                         [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-3                       [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-4                         [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-5                       [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-6                         [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-7                       [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-8                         [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-9                       [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-10                        [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-11                      [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-12                        [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-13                      [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-14                        [32, 64, 2114]            --\n",
       "├─ModuleList: 1-17                       --                        (recursive)\n",
       "│    └─Conv1d: 2-15                      [32, 64, 2114]            12,352\n",
       "├─ModuleList: 1-18                       --                        --\n",
       "│    └─ReLU: 2-16                        [32, 64, 2114]            --\n",
       "├─Conv1d: 1-19                           [32, 2, 2114]             9,902\n",
       "├─Linear: 1-20                           [32, 1]                   66\n",
       "==========================================================================================\n",
       "Total params: 114,224\n",
       "Trainable params: 114,224\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.72\n",
       "==========================================================================================\n",
       "Input size (MB): 1.62\n",
       "Forward/backward pass size (MB): 312.80\n",
       "Params size (MB): 0.46\n",
       "Estimated Total Size (MB): 314.89\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, input_size=((32, 4, 2114), (32, 2, 2114)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model(batch[0].cuda(), batch[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 2, 1000]), torch.Size([100, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0].shape, outs[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_train, y_toy_train, X_ctl_toy_train = eu.dl.read_profile(peaks, seqs, signals, controls, max_jitter=128, chroms=training_chroms)\n",
    "X_toy_val, y_toy_val, X_ctl_toy_val = eu.dl.read_profile(peaks, seqs, signals, controls, max_jitter=0, chroms=valid_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([45803, 4, 2370]),\n",
       " torch.Size([45803, 2, 1256]),\n",
       " torch.Size([45803, 2, 2370]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy_train.shape, y_toy_train.shape, X_ctl_toy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7051, 4, 2114]),\n",
       " torch.Size([7051, 2, 1000]),\n",
       " torch.Size([7051, 2, 2114]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy_val.shape, y_toy_val.shape, X_ctl_toy_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.dataload import ProfileDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_dataset_train = ProfileDataset(\n",
    "    X_toy_train,\n",
    "    y_toy_train,\n",
    "    X_ctl_toy_train\n",
    ")\n",
    "X_toy_dataset_val = ProfileDataset(\n",
    "    X_toy_val,\n",
    "    y_toy_val,\n",
    "    X_ctl_toy_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_dataloader_train = X_toy_dataset_train.to_dataloader(batch_size=64, workers=4, shuffle=True)\n",
    "X_toy_dataloader_val = X_toy_dataset_val.to_dataloader(batch_size=64, workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene.models.base._ProfileModel import ProfileModel\n",
    "from eugene.models._profile_models import BPNet\n",
    "\n",
    "model = BPNet(\n",
    "    input_len=2114,\n",
    "    output_dim=1000,\n",
    "    n_outputs=2,\n",
    "    n_control_tracks=2, \n",
    "    trimming=(2114 - 1000) // 2\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model(batch[0].cuda(), batch[1].cuda())\n",
    "outs[0].shape, outs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=1, logger=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | iconv  | Conv1d     | 5.4 K \n",
      "1 | irelu  | ReLU       | 0     \n",
      "2 | rconvs | ModuleList | 98.8 K\n",
      "3 | rrelus | ModuleList | 0     \n",
      "4 | fconv  | Conv1d     | 9.9 K \n",
      "5 | linear | Linear     | 66    \n",
      "--------------------------------------\n",
      "114 K     Trainable params\n",
      "0         Non-trainable params\n",
      "114 K     Total params\n",
      "0.457     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f888bd3906415a8640a71c48d040f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cellar/users/aklie/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f66bea78174f66863a05e49ff571e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee87c6939bd41c6a4f0423d22445a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, X_toy_dataloader_train, X_toy_dataloader_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqexplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mseqexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/projects/ML4GLand/SeqExplainer/seqexplainer/_feature_attribution.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "seqexplainer.attribute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4083192/4061347828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"InputXGradient\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ctl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/projects/ML4GLand/SeqExplainer/seqexplainer/_feature_attribution.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(model, inputs, method, target, device, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ML4GLand/SeqExplainer/seqexplainer/_feature_attribution.py\u001b[0m in \u001b[0;36m_captum_attributions\u001b[0;34m(model, inputs, method, target, device, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mattributor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCAPTUM_REGISTRY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattributor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/input_x_gradient.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         gradients = self.gradient_func(\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# runs forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         assert outputs[0].numel() == 1, (\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m\"Target not provided when necessary, cannot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     )\n\u001b[0;32m--> 461\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_select_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_select_targets\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "seqexplainer.attribute(\n",
    "    pretrained_model,\n",
    "    X,\n",
    "    method=\"InputXGradient\",\n",
    "    additional_forward_args=(X_ctl,),\n",
    "    target=0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 eugene_dev",
   "language": "python",
   "name": "eugene_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
