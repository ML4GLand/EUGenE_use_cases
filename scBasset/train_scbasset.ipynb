{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scBasset on PBMC dataset described at scBasset GitHub page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:24:42.303617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:42.303641: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-06 16:24:59.352523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:24:59.362549: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.370928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.379255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.387456: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.396155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.403873: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.411382: W tensorflow/stream_executor/p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "latform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.417726: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/slurm/current/lib64/slurm:/cm/shared/apps/slurm/current/lib64\n",
      "2023-04-06 16:24:59.417736: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "import psutil\n",
    "import anndata\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "\n",
    "# see ig GPU is available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run this to create the scBasset input files for the PBMC dataset.\\n%%bash\\nsource activate scbasset\\npython /cellar/users/aklie/opt/ml4gland/scBasset/bin/scbasset_preprocess.py     --ad_file /cellar/users/aklie/data/ml4gland/use_cases/yuan22/github_tutorial/multiome_pbmc/atac_ad.h5ad     --input_fasta /cellar/users/aklie/data/ml4gland/genomes/hg38/hg38.fa     --out_path /cellar/users/aklie/data/ml4gland/use_cases/yuan22/github_tutorial/multiome_pbmc/processed '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Run this to create the scBasset input files for the PBMC dataset.\n",
    "%%bash\n",
    "source activate scbasset\n",
    "python /cellar/users/aklie/opt/ml4gland/scBasset/bin/scbasset_preprocess.py \\\n",
    "    --ad_file /cellar/users/aklie/data/ml4gland/use_cases/yuan22/github_tutorial/multiome_pbmc/atac_ad.h5ad \\\n",
    "    --input_fasta /cellar/users/aklie/data/ml4gland/genomes/hg38/hg38.fa \\\n",
    "    --out_path /cellar/users/aklie/data/ml4gland/use_cases/yuan22/github_tutorial/multiome_pbmc/processed \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/cellar/users/aklie/data/ml4gland/use_cases/yuan22/github_tutorial/multiome_pbmc'\n",
    "split_file = os.path.join(data_dir, 'processed', 'splits.h5')\n",
    "train_file = os.path.join(data_dir, 'processed', 'train_seqs.h5')\n",
    "val_file = os.path.join(data_dir, 'processed', 'val_seqs.h5')\n",
    "test_file = os.path.join(data_dir, 'processed', 'test_seqs.h5')\n",
    "ad_file = os.path.join(data_dir, 'atac_ad.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a generator to read examples from h5 file\n",
    "# create a tf dataset\n",
    "class generator:\n",
    "    def __init__(self, file, m):\n",
    "        self.file = file # h5 file for sequence\n",
    "        self.m = m # csr matrix, rows as seqs, cols are cells\n",
    "        self.n_cells = m.shape[1]\n",
    "        self.ones = np.ones(1344)\n",
    "        self.rows = np.arange(1344)\n",
    "\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "            X = hf['X']\n",
    "            for i in range(X.shape[0]):\n",
    "                x = X[i]\n",
    "                x_tf = sparse.coo_matrix((self.ones, (self.rows, x)), \n",
    "                                               shape=(1344, 4), \n",
    "                                               dtype='int8').toarray()\n",
    "                y = self.m.indices[self.m.indptr[i]:self.m.indptr[i+1]]\n",
    "                y_tf = np.zeros(self.n_cells, dtype='int8')\n",
    "                y_tf[y] = 1\n",
    "                yield x_tf, y_tf\n",
    "\n",
    "def print_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print('cpu memory used: %.1fGB.'%(process.memory_info().rss/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the sparse matrix from the anndata object\n",
    "adata = anndata.read_h5ad(ad_file)\n",
    "n_cells = adata.shape[0]\n",
    "m = adata.X.tocoo().transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu memory used: 0.7GB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_memory()     # memory usage\n",
    "del adata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the splits\n",
    "with h5py.File(split_file, 'r') as hf:\n",
    "    train_ids = hf['train_ids'][:]\n",
    "    val_ids = hf['val_ids'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24436, 2714), (1357, 2714))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split into train and val\n",
    "m_train = m[train_ids,:]\n",
    "m_val = m[val_ids,:]\n",
    "del m\n",
    "gc.collect()\n",
    "m_train.shape, m_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf datasets\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "        generator(train_file, m_train), \n",
    "        output_signature=(\n",
    "             tf.TensorSpec(shape=(1344,4), dtype=tf.int8),\n",
    "             tf.TensorSpec(shape=(n_cells), dtype=tf.int8),\n",
    "        )\n",
    "    ).shuffle(2000, reshuffle_each_iteration=True).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_generator(\n",
    "        generator(val_file, m_val), \n",
    "        output_signature=(\n",
    "             tf.TensorSpec(shape=(1344,4), dtype=tf.int8),\n",
    "             tf.TensorSpec(shape=(n_cells), dtype=tf.int8),\n",
    "        )\n",
    "    ).batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 1344, 4) (128, 2714)\n"
     ]
    }
   ],
   "source": [
    "# Get an example batch from training dataset\n",
    "for x, y in train_ds.take(1):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an scBasset model with their code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scbasset.utils import make_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:14:11.749601: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 1344, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 1344, 4),   0           ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)   ())                                                              \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 1344, 4)     0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " gelu (GELU)                    (None, 1344, 4)      0           ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 1344, 288)    19584       ['gelu[0][0]']                   \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1344, 288)   1152        ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 448, 288)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " gelu_1 (GELU)                  (None, 448, 288)     0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 448, 288)     414720      ['gelu_1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 448, 288)    1152        ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 224, 288)    0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_2 (GELU)                  (None, 224, 288)     0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 224, 323)     465120      ['gelu_2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 224, 323)    1292        ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 112, 323)    0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_3 (GELU)                  (None, 112, 323)     0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 112, 363)     586245      ['gelu_3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 112, 363)    1452        ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 56, 363)     0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_4 (GELU)                  (None, 56, 363)      0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 56, 407)      738705      ['gelu_4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 56, 407)     1628        ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 28, 407)     0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_5 (GELU)                  (None, 28, 407)      0           ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 28, 456)      927960      ['gelu_5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 28, 456)     1824        ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 14, 456)     0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_6 (GELU)                  (None, 14, 456)      0           ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 14, 512)      1167360     ['gelu_6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 14, 512)     2048        ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 7, 512)      0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_7 (GELU)                  (None, 7, 512)       0           ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 7, 256)       131072      ['gelu_7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 7, 256)      1024        ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " gelu_8 (GELU)                  (None, 7, 256)       0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 1792)      0           ['gelu_8[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1, 32)        57344       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 1, 32)       128         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1, 32)        0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " gelu_9 (GELU)                  (None, 1, 32)        0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1, 2714)      89562       ['gelu_9[0][0]']                 \n",
      "                                                                                                  \n",
      " switch_reverse (SwitchReverse)  (None, 1, 2714)     0           ['dense_1[0][0]',                \n",
      "                                                                  'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2714)         0           ['switch_reverse[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,609,372\n",
      "Trainable params: 4,603,522\n",
      "Non-trainable params: 5,850\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model(32, n_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss, optimizer, and compile the mdodel\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.95,beta_2=0.9995)\n",
    "model.compile(loss=loss_fn, optimizer=optimizer,\n",
    "                metrics=[tf.keras.metrics.AUC(curve='ROC', multi_label=True),\n",
    "                        tf.keras.metrics.AUC(curve='PR', multi_label=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(data_dir, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# earlystopping, track train AUC\n",
    "filepath = '%s/best_model.h5'%out_dir\n",
    "    \n",
    "# tensorboard\n",
    "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(out_dir),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath, save_best_only=True, \n",
    "                                           save_weights_only=True, monitor='auc', mode='max'),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='auc', min_delta=1e-6, \n",
    "                                         mode='max', patience=50, verbose=1),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=1000,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_ds)\n",
    "pickle.dump(history.history, open('%s/history.pickle'%out_dir, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
