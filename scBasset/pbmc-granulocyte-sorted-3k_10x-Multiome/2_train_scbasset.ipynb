{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train scBasset model on `pbmc-granulocyte-sorted-3k_10x-Multiome` dataset using scBasset\n",
    "Adam Klie (last updated: *09/20/2023*)\n",
    "***\n",
    "This notebook shows how to train models using scBasset. This notebook uses bits and pieces of the scbasset package code, allowing for more flexibility in the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "import psutil\n",
    "import anndata\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scbasset.utils import make_model\n",
    "\n",
    "# see ig GPU is available\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a generator to read examples from h5 file to create a tf dataset\n",
    "class generator:\n",
    "    def __init__(self, file, m):\n",
    "        self.file = file # h5 file for sequence\n",
    "        self.m = m # csr matrix, rows as seqs, cols are cells\n",
    "        self.n_cells = m.shape[1]\n",
    "        self.ones = np.ones(1344)\n",
    "        self.rows = np.arange(1344)\n",
    "\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as hf:\n",
    "            X = hf['X']\n",
    "            for i in range(X.shape[0]):\n",
    "                x = X[i]\n",
    "                x_tf = sparse.coo_matrix((self.ones, (self.rows, x)), \n",
    "                                               shape=(1344, 4), \n",
    "                                               dtype='int8').toarray()\n",
    "                y = self.m.indices[self.m.indptr[i]:self.m.indptr[i+1]]\n",
    "                y_tf = np.zeros(self.n_cells, dtype='int8')\n",
    "                y_tf[y] = 1\n",
    "                yield x_tf, y_tf\n",
    "\n",
    "def print_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print('cpu memory used: %.1fGB.'%(process.memory_info().rss/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up the paths to data (TODO: change to your own paths)\n",
    "input_dir = '/cellar/users/aklie/projects/ML4GLand/use_cases/scBasset/pbmc-granulocyte-sorted-3k_10x-Multiome/processed'\n",
    "split_file = os.path.join(input_dir, 'splits.h5')\n",
    "train_file = os.path.join(input_dir, 'train_seqs.h5')\n",
    "val_file = os.path.join(input_dir, 'val_seqs.h5')\n",
    "test_file = os.path.join(input_dir, 'test_seqs.h5')\n",
    "ad_file = os.path.join(input_dir, 'atac_ad.h5ad')\n",
    "output_dir = '/cellar/users/aklie/projects/ML4GLand/use_cases/scBasset/pbmc-granulocyte-sorted-3k_10x-Multiome/model'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the sparse matrix from the anndata object\n",
    "adata = anndata.read_h5ad(ad_file)\n",
    "n_cells = adata.shape[0]\n",
    "m = adata.X.tocoo().transpose().tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage\n",
    "print_memory()\n",
    "del adata\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the splits\n",
    "with h5py.File(split_file, 'r') as hf:\n",
    "    train_ids = hf['train_ids'][:]\n",
    "    val_ids = hf['val_ids'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and val\n",
    "m_train = m[train_ids,:]\n",
    "m_val = m[val_ids,:]\n",
    "del m\n",
    "gc.collect()\n",
    "m_train.shape, m_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf datasets\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "     generator(train_file, m_train), \n",
    "     output_signature=(\n",
    "          tf.TensorSpec(shape=(1344,4), dtype=tf.int8),\n",
    "          tf.TensorSpec(shape=(n_cells), dtype=tf.int8),\n",
    "     )\n",
    ").shuffle(2000, reshuffle_each_iteration=True).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "     generator(val_file, m_val), \n",
    "     output_signature=(\n",
    "          tf.TensorSpec(shape=(1344,4), dtype=tf.int8),\n",
    "          tf.TensorSpec(shape=(n_cells), dtype=tf.int8),\n",
    "     )\n",
    ").batch(128).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an example batch from training dataset\n",
    "for x, y in train_ds.take(1):\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an scBasset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(32, n_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss, optimizer, and compile the mdodel\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.95,beta_2=0.9995)\n",
    "model.compile(\n",
    "    loss=loss_fn, \n",
    "    optimizer=optimizer,\n",
    "    metrics=[tf.keras.metrics.AUC(curve='ROC', multi_label=True),\n",
    "    tf.keras.metrics.AUC(curve='PR', multi_label=True)]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earlystopping, track train AUC\n",
    "filepath = os.path.join(output_dir, 'best_model.h5')\n",
    "    \n",
    "# tensorboard\n",
    "logs = os.path.join(output_dir, \"logs\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(output_dir)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    monitor='auc', \n",
    "    mode='max'\n",
    ")\n",
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='auc', \n",
    "    min_delta=1e-6, \n",
    "    mode='max', \n",
    "    patience=50, \n",
    "    verbose=1\n",
    ")\n",
    "callbacks = [tensorboard_callback, checkpoint_callback, earlystopping_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=1000,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(history.history, open('%s/history.pickle'%output_dir, 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with script\n",
    "You can also train a model using the scbasset_train.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate scbasset\n",
    "python /cellar/users/aklie/opt/ml4gland/scBasset/bin/scbasset_train.py \\\n",
    "    --input_folder /cellar/users/aklie/projects/ML4GLand/use_cases/scBasset/pbmc-granulocyte-sorted-3k_10x-Multiome/processed \\\n",
    "    --out_path /cellar/users/aklie/projects/ML4GLand/use_cases/scBasset/pbmc-granulocyte-sorted-3k_10x-Multiome/model/21Sep23/scbasset/script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 scbasset",
   "language": "python",
   "name": "scbasset"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
